predicate isDimensional(string fullyQualifiedName) {
  fullyQualifiedName in [
      "org.apache.hadoop.ipc.Server$ConnectionManager$1.run",
      "org.apache.hadoop.hdfs.DFSUtilClient.compareBytes",
      "org.apache.hadoop.hdfs.server.namenode.NameNodeResourcePolicy.areResourcesAvailable",
      "org.apache.hadoop.hdfs.server.namenode.LeaseManager.checkLeases",
      "org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors",
      "org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkBlocksComplete",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processIncrementalBlockReport",
      "org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream.shouldForceSync",
      "org.apache.hadoop.hdfs.server.namenode.INodeFile.storagespaceConsumedContiguous",
      "org.apache.hadoop.hdfs.server.namenode.FSEditLog.endTransaction",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.processQueue",
      "org.apache.hadoop.hdfs.server.namenode.INodeFile.assertAllBlocksComplete",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkRedundancy",
      "org.apache.hadoop.util.JvmPauseMonitor.getGcTimes",
      "org.apache.hadoop.io.ArrayWritable.write",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.countNodes",
      "org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync.dequeueEdit",
      "org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync",
      "org.apache.hadoop.hdfs.server.namenode.INodeFile.computeFileSize",
      "org.apache.hadoop.hdfs.util.FoldedTreeSet$Node.getRightMostNode",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.findStorageInfo",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getPipeline",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.isGoodDatanode",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNodeUsage",
      "org.apache.hadoop.net.NetworkTopology.chooseRandom",
      "org.apache.hadoop.hdfs.net.DFSNetworkTopology.chooseRandomWithStorageType",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getVolumeFailuresTotal",
      "org.apache.hadoop.hdfs.protocol.BlockStoragePolicy.chooseStorageTypes",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNumInMaintenanceLiveDataNodes",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto.writeTo",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.resetLastCachingDirectiveSentTime",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom",
      "org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap.getDatanodeByXferAddr",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeListForReport",
      "org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.rescanCachedBlockMap",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.addExpectedReplicasToPending",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto.writeTo",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toDatanodeDescriptors",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.scheduleReconstruction",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto.isInitialized",
      "org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.incrementBlocksScheduled",
      "org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager.heartbeatCheck",
      "org.apache.hadoop.hdfs.net.DFSNetworkTopology.chooseRandomWithStorageTypeTwoTrial",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature.addReplicaIfNotPresent",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature.setExpectedLocations",
      "org.apache.hadoop.net.InnerNodeImpl.getLeaf",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.removeDecomNodeFromList",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEstimatedCapacityLostTotal",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto.getSerializedSize",
      "org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertLocatedBlock",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.readUnlock",
      "org.apache.hadoop.hdfs.protocol.BlockStoragePolicy.diff",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getLiveNodes",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.chooseStorage4Block",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.getRequiredStorageTypes",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.validateReconstructionWork",
      "org.apache.hadoop.hdfs.util.LightWeightLinkedSet.addElem",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getNumLiveDataNodes",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.fetchDatanodes",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.numBlocks",
      "org.apache.hadoop.hdfs.util.EnumCounters.sum",
      "org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl.add",
      "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks.add",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toDatanodeInfos",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature.getStaleReplicas",
      "org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks.decrement",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature.appendUCPartsConcise",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageIDs",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseSourceDatanodes",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageTypes",
      "org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap.contains",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder.addAllStorageIDs",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature.getExpectedStorageLocations",
      "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.hasStorageType",
      "org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNumDecomLiveDataNodes",
      "org.apache.hadoop.hdfs.util.LightWeightHashSet.resize",
      "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReconstructionWorkForBlocks",
      "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks.chooseLowRedundancyBlocks",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto.writeTo",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto.writeTo",
      "org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convertStorageTypes",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto.writeTo",
      "org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor.pendingReconstructionCheck",
      "org.apache.hadoop.ipc.Server$Listener.run",
      "org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert",
      "org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.initChildren",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto.getSerializedSize",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto.writeTo",
      "org.apache.hadoop.ipc.Server$Listener.doAccept",
      "org.apache.hadoop.ipc.Server$Listener.doRead",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder.addAllDiffReportEntries",
      "org.apache.hadoop.hdfs.DFSUtilClient.byteArray2bytes",
      "org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo.addDirDiff",
      "org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo.generateReport",
      "org.apache.hadoop.hdfs.util.Diff.apply2Previous",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto.isInitialized",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto$Builder.addAllStorageUuids",
      "org.apache.hadoop.ipc.Server$ConnectionManager.register",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto.writeTo",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto.getSerializedSize",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto$Builder.addAllDatanodes",
      "org.apache.hadoop.util.LightWeightCache.evictExpiredEntries",
      "org.apache.hadoop.hdfs.util.Diff.combinePosterior",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto.isInitialized",
      "org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.computeDiffBetweenSnapshots",
      "org.apache.hadoop.hdfs.util.EnumCounters.reset",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto.getSerializedSize",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto$Builder.addAllStorageTypes",
      "org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto.getSerializedSize"
    ]
}
